{
	"jobConfig": {
		"name": "glueworkshop-lab3-etl-ddb-job",
		"description": "",
		"role": "arn:aws:iam::123456789012:role/AWSGlueServiceRole-glueworkshop",
		"command": "glueetl",
		"version": "3.0",
		"runtime": null,
		"workerType": "G.1X",
		"numberOfWorkers": "5",
		"maxCapacity": 5,
		"maxRetries": "0",
		"timeout": 2880,
		"maxConcurrentRuns": 1,
		"security": "none",
		"scriptName": "glueworkshop-lab3-etl-ddb-job.py",
		"scriptLocation": "s3://aws-glue-assets-123456789012-us-east-2/scripts/",
		"language": "python-3",
		"jobParameters": [],
		"tags": [],
		"jobMode": "NOTEBOOK_MODE",
		"createdOn": "2023-10-18T06:49:27.042Z",
		"developerMode": false,
		"connectionsList": [],
		"temporaryDirectory": "s3://aws-glue-assets-123456789012-us-east-2/temporary/",
		"etlAutoScaling": "false",
		"glueHiveMetastore": "true",
		"etlAutoTuning": "false",
		"spark": "true",
		"serverEncryption": "false",
		"dependentPath": "s3://crawler-public/json/serde/json-serde.jar",
		"pythonPath": "s3://glueworkshop-123456789012-us-east-2/library/pycountry_convert.zip",
		"sparkPath": "s3://glueworkshop-123456789012-us-east-2/output/lab3/sparklog/",
		"pythonShellPrebuiltLibraryOption": "analytics",
		"flexExecution": false,
		"minFlexWorkers": null,
		"bookmark": "",
		"metrics": "",
		"logging": "",
		"referencedPath": "",
		"etlAutoTuningJobRules": "",
		"serverlessSparkUI": "",
		"pythonVersion": ""
	},
	"hasBeenSaved": false,
	"script": "#Importing all the basic Glue, Spark libraries \n\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n\n# Important further required libraries\n\nimport os, sys, boto3\nfrom pprint import pprint\nfrom awsglue.dynamicframe import DynamicFrame\nfrom pyspark.sql.functions import udf, col\nfrom pyspark.sql.types import IntegerType, StringType\nfrom datetime import datetime\n\n# Starting Spark/Glue Context\n\nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)\n\n# Important pycountry_convert function from the external python library (pycountry_convert.zip)\n\nfrom pycountry_convert import (\n    convert_country_alpha2_to_country_name,\n    convert_country_alpha2_to_continent,\n    convert_country_name_to_country_alpha2,\n    convert_country_alpha3_to_country_alpha2,\n)\n\n# Defining the function code\n\ndef get_country_code2(country_name):\n    country_code2 = 'US'\n    try:\n        country_code2 = convert_country_name_to_country_alpha2(country_name)\n    except KeyError:\n        country_code2 = ''\n    return country_code2\n\nudf_get_country_code2 = udf(lambda z: get_country_code2(z), StringType())\n\n#Get parameter values\n\ns3_bucket_name = \"s3://glueworkshop-123456789012-us-east-2/\"                              # <------- PLEASE REPLACE ONLY THE ${BUCKET_NAME} HERE (Keep the \"s3://\" and the final \"/\" part)!!!\nregion_name = 'us-east-2'                                        #  <--- REPLACE THE AWS REGION\nddb_table_name='glueworkshop-lab3'\n\n\n# Create the dynamodb with appropriate read and write capacity\n# Get service resource\ndynamodb = boto3.resource('dynamodb', region_name=region_name)\n\ntable_status = dynamodb.create_table(\n    TableName=ddb_table_name,\n    KeySchema=[{'AttributeName': 'uuid','KeyType': 'HASH'}],\n    AttributeDefinitions=[{'AttributeName': 'uuid','AttributeType': 'N'}],\n    ProvisionedThroughput={'ReadCapacityUnits': 500,'WriteCapacityUnits': 5000}\n    )\n# Wait until the table exists.\ntable_status.meta.client.get_waiter('table_exists').wait(TableName=ddb_table_name)\npprint(table_status)\n\ndf = spark.read.load(s3_bucket_name + \"input/lab2/sample.csv\", \n                     format=\"csv\", \n                     sep=\",\", \n                     inferSchema=\"true\", \n                     header=\"true\")\n\n\nnew_df = df.withColumn('country_code_2', udf_get_country_code2(col(\"Country\")))\nnew_df_dyf=DynamicFrame.fromDF(new_df, glueContext, \"new_df_dyf\")\n\nprint(\"Start writing to DBB : {}\".format(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")))\nglueContext.write_dynamic_frame_from_options(\n    frame=new_df_dyf,\n    connection_type=\"dynamodb\",\n    connection_options={\n        \"dynamodb.output.tableName\": ddb_table_name,\n        \"dynamodb.throughput.write.percent\": \"1.0\"\n    }\n)\nprint(\"Finished writing to DBB : {}\".format(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")))\n\n# Comparing Counts\n    \nnew_df.count()\n\njob.commit()",
	"notebook": {
		"metadata": {
			"kernelspec": {
				"name": "glue_pyspark",
				"display_name": "Glue PySpark",
				"language": "python"
			},
			"language_info": {
				"name": "Python_Glue_Session",
				"mimetype": "text/x-python",
				"codemirror_mode": {
					"name": "python",
					"version": 3
				},
				"pygments_lexer": "python3",
				"file_extension": ".py"
			}
		},
		"nbformat_minor": 4,
		"nbformat": 4,
		"cells": [
			{
				"cell_type": "code",
				"source": "# Adding required libraries and extra jars to the job -   # <------- PLEASE REPLACE ${BUCKET_NAME} BELOW!!!\n\n%extra_py_files s3://glueworkshop-123456789012-us-east-2/library/pycountry_convert.zip\n%extra_jars s3://crawler-public/json/serde/json-serde.jar\n\n# Adding required properties to the job - # <------- PLEASE REPLACE ${BUCKET_NAME} BELOW!!!\n\n%%configure \n{\n  \"--enable-spark-ui\": \"true\",\n  \"--spark-event-logs-path\": \"s3://glueworkshop-123456789012-us-east-2/output/lab3/sparklog/\",\n  \"max_retries\": \"0\"         \n}\n",
				"metadata": {
					"trusted": true,
					"tags": []
				},
				"execution_count": 4,
				"outputs": [
					{
						"name": "stdout",
						"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 0.38.1 \nExtra py files to be included:\ns3://glueworkshop-123456789012-us-east-2/library/pycountry_convert.zip\nExtra jars to be included:\ns3://crawler-public/json/serde/json-serde.jar\nThe following configurations have been updated: {'--enable-spark-ui': 'true', '--spark-event-logs-path': 's3://glueworkshop-123456789012-us-east-2/output/lab3/sparklog/', 'max_retries': '0'}\ns3://crawler-public/json/serde/json-serde.jar\n",
						"output_type": "stream"
					}
				]
			},
			{
				"cell_type": "code",
				"source": "#Importing all the basic Glue, Spark libraries \n\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n\n# Important further required libraries\n\nimport os, sys, boto3\nfrom pprint import pprint\nfrom awsglue.dynamicframe import DynamicFrame\nfrom pyspark.sql.functions import udf, col\nfrom pyspark.sql.types import IntegerType, StringType\nfrom datetime import datetime\n\n# Starting Spark/Glue Context\n\nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)\n\n# Important pycountry_convert function from the external python library (pycountry_convert.zip)\n\nfrom pycountry_convert import (\n    convert_country_alpha2_to_country_name,\n    convert_country_alpha2_to_continent,\n    convert_country_name_to_country_alpha2,\n    convert_country_alpha3_to_country_alpha2,\n)\n\n# Defining the function code\n\ndef get_country_code2(country_name):\n    country_code2 = 'US'\n    try:\n        country_code2 = convert_country_name_to_country_alpha2(country_name)\n    except KeyError:\n        country_code2 = ''\n    return country_code2\n\nudf_get_country_code2 = udf(lambda z: get_country_code2(z), StringType())\n",
				"metadata": {
					"trusted": true,
					"tags": []
				},
				"execution_count": 1,
				"outputs": [
					{
						"name": "stdout",
						"text": "Authenticating with environment variables and user-defined glue_role_arn: arn:aws:iam::123456789012:role/AWSGlueServiceRole-glueworkshop\nTrying to create a Glue session for the kernel.\nWorker Type: G.1X\nNumber of Workers: 5\nSession ID: ccbe5d58-1d6d-4938-9f34-0ba06d9b8cac\nJob Type: glueetl\nApplying the following default arguments:\n--glue_kernel_version 0.38.1\n--enable-glue-datacatalog true\n--extra-py-files s3://glueworkshop-123456789012-us-east-2/library/pycountry_convert.zip\n--extra-jars s3://crawler-public/json/serde/json-serde.jar\n--enable-spark-ui true\n--spark-event-logs-path s3://glueworkshop-123456789012-us-east-2/output/lab3/sparklog/\n--max_retries 0\nWaiting for session ccbe5d58-1d6d-4938-9f34-0ba06d9b8cac to get into ready status...\nSession ccbe5d58-1d6d-4938-9f34-0ba06d9b8cac has been created.\n\n",
						"output_type": "stream"
					}
				]
			},
			{
				"cell_type": "code",
				"source": "#Get parameter values\n\ns3_bucket_name = \"s3://glueworkshop-123456789012-us-east-2/\"                              # <------- PLEASE REPLACE ONLY THE ${BUCKET_NAME} HERE (Keep the \"s3://\" and the final \"/\" part)!!!\nregion_name = 'us-east-2'                                        #  <--- REPLACE THE AWS REGION\nddb_table_name='glueworkshop-lab3'\n\n\n# Create the dynamodb with appropriate read and write capacity\n# Get service resource\ndynamodb = boto3.resource('dynamodb', region_name=region_name)\n\ntable_status = dynamodb.create_table(\n    TableName=ddb_table_name,\n    KeySchema=[{'AttributeName': 'uuid','KeyType': 'HASH'}],\n    AttributeDefinitions=[{'AttributeName': 'uuid','AttributeType': 'N'}],\n    ProvisionedThroughput={'ReadCapacityUnits': 500,'WriteCapacityUnits': 5000}\n    )\n# Wait until the table exists.\ntable_status.meta.client.get_waiter('table_exists').wait(TableName=ddb_table_name)\npprint(table_status)\n\ndf = spark.read.load(s3_bucket_name + \"input/lab2/sample.csv\", \n                     format=\"csv\", \n                     sep=\",\", \n                     inferSchema=\"true\", \n                     header=\"true\")\n\n\nnew_df = df.withColumn('country_code_2', udf_get_country_code2(col(\"Country\")))\nnew_df_dyf=DynamicFrame.fromDF(new_df, glueContext, \"new_df_dyf\")\n\nprint(\"Start writing to DBB : {}\".format(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")))\nglueContext.write_dynamic_frame_from_options(\n    frame=new_df_dyf,\n    connection_type=\"dynamodb\",\n    connection_options={\n        \"dynamodb.output.tableName\": ddb_table_name,\n        \"dynamodb.throughput.write.percent\": \"1.0\"\n    }\n)\nprint(\"Finished writing to DBB : {}\".format(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")))\n\n# Comparing Counts\n    \nnew_df.count()\n",
				"metadata": {
					"trusted": true,
					"tags": []
				},
				"execution_count": 2,
				"outputs": [
					{
						"name": "stdout",
						"text": "dynamodb.Table(name='glueworkshop-lab3')\nStart writing to DBB : 2023-10-18 06:52:30\nFinished writing to DBB : 2023-10-18 06:52:57\n100000\n",
						"output_type": "stream"
					}
				]
			},
			{
				"cell_type": "code",
				"source": "",
				"metadata": {},
				"execution_count": null,
				"outputs": []
			}
		]
	}
}